#python stuff
import pandas as pd
import seaborn as sb
import numpy as np
from math import floor
from matplotlib import pyplot as plt

#Sklearn stuff
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import classification_report, confusion_matrix

#torch stuff
import torch
from torchvision.models import vgg16, VGG16_Weights, vit_b_16
from cuda_selector import auto_cuda
from torch.nn.functional import softmax as sm
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler

class ParametrizableCNN(nn.Module):
    def __init__(self, input_height=100, input_width=None, num_channels=1, 
                 conv_channels=[32, 64,128], kernel_sizes=[3, 3, 3], 
                 fc_hidden_size=128, dropout_rate=0.5):
        """
        Parametrizable CNN for binary classification with variable input dimensions.
        
        Args:
            input_height (int): Fixed height dimension (default: 100)
            input_width (int): Variable width dimension
            num_channels (int): Number of input channels
            conv_channels (list): Number of channels for each conv layer
            kernel_sizes (list): Kernel sizes for each conv layer
            fc_hidden_size (int): Hidden size for fully connected layer
            dropout_rate (float): Dropout rate
        """
        super(ParametrizableCNN, self).__init__()
        
        self.input_height = input_height
        self.input_width = input_width
        self.dropout_rate = dropout_rate
        
        # Convolutional layers
        self.conv_layers = nn.ModuleList()
        in_channels = num_channels
        
        for i, (out_channels, kernel_size) in enumerate(zip(conv_channels, kernel_sizes)):
            conv_layer = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1),
                nn.BatchNorm2d(out_channels),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(2, 2)
            )
            self.conv_layers.append(conv_layer)
            in_channels = out_channels
        
        # Calculate the size after convolutions
        self.feature_size = self._calculate_conv_output_size(input_height, input_width, len(conv_channels))
        self.final_channels = conv_channels[-1]
        
        # Fully connected layers
        self.fc1 = nn.Linear(self.final_channels * self.feature_size, fc_hidden_size)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(fc_hidden_size, 2)  # Binary classification

    def _calculate_conv_output_size(self, h, w, num_pools):
        """Calculate output size after convolutions and pooling"""
        for _ in range(num_pools):
            h = h // 2
            w = w // 2
        return h * w
    
    def forward(self, x):
        # Apply convolutional layers
        for conv_layer in self.conv_layers:
            x = conv_layer(x)
        
        # Flatten for fully connected layers
        x = x.view(x.size(0), -1)
        
        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = self.fc2(x)
        
        return x
    
class ImbalancedBinaryClassificationTrainer:
    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
        
    def calculate_class_weights(self, y_train):
        """Calculate class weights for imbalanced dataset"""
        # Convert to integers if needed for sklearn compatibility
        y_train_int = y_train.astype(int) if y_train.dtype != int else y_train
        
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(y_train_int),
            y=y_train_int
        )
        return torch.FloatTensor(class_weights).to(self.device)
    
    def create_weighted_sampler(self, y_train):
        """Create weighted sampler for imbalanced dataset"""
        # Convert to integers if needed
        y_train_int = y_train.astype(int) if y_train.dtype != int else y_train
        
        class_counts = np.bincount(y_train_int)
        class_weights = 1.0 / class_counts
        sample_weights = class_weights[y_train_int]
        
        return WeightedRandomSampler(sample_weights, len(sample_weights))
    
    def train_epoch(self, train_loader, optimizer, criterion):
        """Train for one epoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            optimizer.zero_grad()
            output = self.model(data).squeeze()
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            predicted = (torch.sigmoid(output) > 0.5).float()
            total += target.size(0)
            correct += (predicted == target).sum().item()
            
            if batch_idx % 100 == 0:
                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        return epoch_loss, epoch_acc
    
    def validate_epoch(self, val_loader, criterion):
        """Validate for one epoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device).float().long()
                output = self.model(data).squeeze()
                loss = criterion(output, target)
                
                running_loss += loss.item()
                predicted = (torch.sigmoid(output) > 0.5).float()
                total += target.size(0)
                correct += (predicted == target).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader, num_epochs=100, learning_rate=0.001,
              use_class_weights=True, use_weighted_sampling=False, patience=10):
        """
        Train the model with various techniques to handle imbalanced data
        
        Args:
            train_loader: Training data loader
            val_loader: Validation data loader
            num_epochs: Number of training epochs
            learning_rate: Learning rate for optimizer
            use_class_weights: Whether to use class weights in loss function
            use_weighted_sampling: Whether to use weighted sampling (requires train_dataset)
            patience: Early stopping patience
        """
        
        # Setup optimizer
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)
        
        # Handle weighted sampling - need to recreate train_loader if enabled
        if use_weighted_sampling:
            print("Creating weighted sampler for imbalanced training...")
            # Extract labels from original train_loader
            train_labels = []
            for _, labels in train_loader:
                train_labels.extend(labels.numpy())
            train_labels = np.array(train_labels)
            
            # Create weighted sampler
            weighted_sampler = self.create_weighted_sampler(train_labels)
            
            # Get original dataset from the loader
            train_dataset = train_loader.dataset
            
            # Create new loader with weighted sampler (no shuffle when using sampler)
            train_loader = DataLoader(
                train_dataset, 
                batch_size=train_loader.batch_size,
                sampler=weighted_sampler,
                num_workers=getattr(train_loader, 'num_workers', 0)
            )
            print(f"Created weighted sampler with {len(weighted_sampler)} samples")
        
        # Setup loss function
        if use_class_weights:
            # Extract labels from train_loader to calculate class weights
            train_labels = []
            for _, labels in train_loader:
                train_labels.extend(labels.numpy())
            class_weights = self.calculate_class_weights(np.array(train_labels))
            criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1]/class_weights[0])
            print(f"Using class weights: {class_weights}")
        else:
            criterion = nn.BCEWithLogitsLoss()
            
        
        # Early stopping variables
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"Training on {self.device}")
        print(f"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
        for epoch in range(num_epochs):
            print(f'\nEpoch {epoch+1}/{num_epochs}')
            print('-' * 50)
            
            # Training
            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)
            
            # Validation
            val_loss, val_acc = self.validate_epoch(val_loader, criterion)
            
            # Store metrics
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            # Learning rate scheduling
            scheduler.step(val_loss)
            
            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
            print(f'Current LR: {optimizer.param_groups[0]["lr"]:.6f}')
            
            # Early stopping
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # Save best model
                torch.save(self.model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f'\nEarly stopping triggered after {epoch+1} epochs')
                    break
        
        # Load best model
        self.model.load_state_dict(torch.load('best_model.pth'))
        print('Training completed!')
    
    def evaluate(self, test_loader, return_scores=False):
        """Evaluate model on test set"""
        self.model.eval()
        all_predictions = []
        all_targets = []
        all_scores = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data).squeeze()
                scores = torch.sigmoid(output)  # Convert logits to probabilities
                predicted = (scores > 0.5).float()
                
                all_predictions.extend(predicted.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_scores.extend(scores.cpu().numpy())
        
        # Print classification report
        print("\nClassification Report:")
        print(classification_report(all_targets, all_predictions, target_names=['Class 0', 'Class 1']))
        
        # Print confusion matrix
        print("\nConfusion Matrix:")
        cm = confusion_matrix(all_targets, all_predictions)
        
        if return_scores:
            return all_predictions, all_targets, all_scores
        else:
            return all_predictions, all_targets
    
    def comprehensive_test_analysis(self, test_loader, threshold=0.5):
        """
        Comprehensive test analysis with accuracy, score distributions, and AUROC
        
        Args:
            test_loader: Test data loader
            threshold: Decision threshold for binary classification
        """
        from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve
        from sklearn.metrics import average_precision_score
        
        self.model.eval()
        all_scores = []
        all_predictions = []
        all_targets = []
        
        print("Running comprehensive test analysis...")
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                # Get raw logits and convert to probabilities
                logits = self.model(data).squeeze()
                probabilities = torch.sigmoid(logits)
                
                # Apply threshold for predictions
                predictions = (probabilities > threshold).float()
                
                # Store results
                all_scores.extend(probabilities.cpu().numpy())
                all_predictions.extend(predictions.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # Convert to numpy arrays
        all_scores = np.array(all_scores)
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        # Calculate metrics
        accuracy = np.mean(all_predictions == all_targets)
        auc_score = roc_auc_score(all_targets, all_scores)
        avg_precision = average_precision_score(all_targets, all_scores)
        
        print(f"\n{'='*60}")
        print(f"COMPREHENSIVE TEST ANALYSIS")
        print(f"{'='*60}")
        print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"AUC-ROC: {auc_score:.4f}")
        print(f"Average Precision (AP): {avg_precision:.4f}")
        
        # Class distribution
        class_0_count = np.sum(all_targets == 0)
        class_1_count = np.sum(all_targets == 1)
        print(f"\nClass Distribution:")
        print(f"Class 0: {class_0_count} samples ({class_0_count/len(all_targets)*100:.1f}%)")
        print(f"Class 1: {class_1_count} samples ({class_1_count/len(all_targets)*100:.1f}%)")
        
        # Detailed classification report
        print(f"\nDetailed Classification Report:")
        print(classification_report(all_targets, all_predictions, 
                                  target_names=['Class 0', 'Class 1'], digits=4))
        
        # Create comprehensive visualization
        self._plot_test_analysis(all_targets, all_scores, all_predictions, 
                               auc_score, avg_precision, threshold)
        
        return {
            'accuracy': accuracy,
            'auc_roc': auc_score,
            'avg_precision': avg_precision,
            'predictions': all_predictions,
            'targets': all_targets,
            'scores': all_scores
        }
    
    def _plot_test_analysis(self, targets, scores, predictions, auc_score, avg_precision, threshold):
        """Create comprehensive visualization of test results"""
        from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix
        import seaborn as sns
        
        # Create figure with subplots
        fig = plt.figure(figsize=(20, 15))
        
        # 1. Score Distribution
        ax1 = plt.subplot(2, 3, 1)
        
        # Separate scores by class
        class_0_scores = scores[targets == 0]
        class_1_scores = scores[targets == 1]
        
        # Plot histograms
        plt.hist(class_0_scores, bins=50, alpha=0.7, label='Class 0', color='red', density=True)
        plt.hist(class_1_scores, bins=50, alpha=0.7, label='Class 1', color='blue', density=True)
        plt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')
        
        plt.xlabel('Prediction Score (Probability)')
        plt.ylabel('Density')
        plt.title('Score Distribution by Class')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add statistics
        plt.text(0.02, 0.95, f'Class 0: μ={np.mean(class_0_scores):.3f}, σ={np.std(class_0_scores):.3f}', 
                transform=ax1.transAxes, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='red', alpha=0.1))
        plt.text(0.02, 0.88, f'Class 1: μ={np.mean(class_1_scores):.3f}, σ={np.std(class_1_scores):.3f}', 
                transform=ax1.transAxes, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='blue', alpha=0.1))
        
        # 2. ROC Curve
        ax2 = plt.subplot(2, 3, 2)
        fpr, tpr, _ = roc_curve(targets, scores)
        plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. Precision-Recall Curve
        ax3 = plt.subplot(2, 3, 3)
        precision, recall, _ = precision_recall_curve(targets, scores)
        plt.plot(recall, precision, linewidth=2, label=f'PR Curve (AP = {avg_precision:.3f})')
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. Confusion Matrix
        ax4 = plt.subplot(2, 3, 4)
        cm = confusion_matrix(targets, predictions)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Class 0', 'Class 1'], 
                   yticklabels=['Class 0', 'Class 1'])
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        
        # 5. Score Box Plot
        ax5 = plt.subplot(2, 3, 5)
        box_data = [class_0_scores, class_1_scores]
        box_plot = plt.boxplot(box_data, labels=['Class 0', 'Class 1'], patch_artist=True)
        box_plot['boxes'][0].set_facecolor('red')
        box_plot['boxes'][0].set_alpha(0.7)
        box_plot['boxes'][1].set_facecolor('blue')
        box_plot['boxes'][1].set_alpha(0.7)
        plt.ylabel('Prediction Score')
        plt.title('Score Distribution Box Plot')
        plt.grid(True, alpha=0.3)
        
        # 6. Class-wise Performance Metrics
        ax6 = plt.subplot(2, 3, 6)
        
        # Calculate per-class metrics
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        precision_0 = precision_score(targets, predictions, pos_label=0)
        precision_1 = precision_score(targets, predictions, pos_label=1)
        recall_0 = recall_score(targets, predictions, pos_label=0)
        recall_1 = recall_score(targets, predictions, pos_label=1)
        f1_0 = f1_score(targets, predictions, pos_label=0)
        f1_1 = f1_score(targets, predictions, pos_label=1)
        
        metrics_data = {
            'Precision': [precision_0, precision_1],
            'Recall': [recall_0, recall_1],
            'F1-Score': [f1_0, f1_1]
        }
        
        x = np.arange(len(metrics_data))
        width = 0.35
        
        class_0_values = [metrics_data['Precision'][0], metrics_data['Recall'][0], metrics_data['F1-Score'][0]]
        class_1_values = [metrics_data['Precision'][1], metrics_data['Recall'][1], metrics_data['F1-Score'][1]]
        
        plt.bar(x - width/2, class_0_values, width, label='Class 0', color='red', alpha=0.7)
        plt.bar(x + width/2, class_1_values, width, label='Class 1', color='blue', alpha=0.7)
        
        plt.xlabel('Metrics')
        plt.ylabel('Score')
        plt.title('Per-Class Performance Metrics')
        plt.xticks(x, ['Precision', 'Recall', 'F1-Score'])
        plt.legend()
        plt.ylim(0, 1.1)
        plt.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for i, (v0, v1) in enumerate(zip(class_0_values, class_1_values)):
            plt.text(i - width/2, v0 + 0.01, f'{v0:.3f}', ha='center', va='bottom')
            plt.text(i + width/2, v1 + 0.01, f'{v1:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        # Print additional insights
        print(f"\n{'='*60}")
        print("INSIGHTS AND RECOMMENDATIONS")
        print(f"{'='*60}")
        
        # Score separation analysis
        score_separation = np.mean(class_1_scores) - np.mean(class_0_scores)
        print(f"Score Separation: {score_separation:.4f}")
        
        if score_separation > 0.3:
            print("✓ Good class separation - model distinguishes well between classes")
        elif score_separation > 0.1:
            print("⚠ Moderate class separation - consider model improvements")
        else:
            print("✗ Poor class separation - model struggles to distinguish classes")
        
        # Threshold analysis
        optimal_threshold_idx = np.argmax(tpr - fpr)
        optimal_threshold = _[optimal_threshold_idx] if len(_) > optimal_threshold_idx else threshold
        print(f"Suggested optimal threshold: {optimal_threshold:.4f} (current: {threshold})")
        
        # Imbalance impact
        if abs(len(class_0_scores) - len(class_1_scores)) / len(scores) > 0.3:
            print("⚠ Significant class imbalance detected - consider using class weights or resampling")
        
        plt.savefig('overall.png')
    
    def plot_training_history(self):
        """Plot training history"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss plot
        ax1.plot(self.train_losses, label='Train Loss')
        ax1.plot(self.val_losses, label='Val Loss')
        ax1.set_title('Loss Over Time')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # Accuracy plot
        ax2.plot(self.train_accuracies, label='Train Acc')
        ax2.plot(self.val_accuracies, label='Val Acc')
        ax2.set_title('Accuracy Over Time')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.show()
        plt.savefig('training_history')